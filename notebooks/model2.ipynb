{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id               object\n",
      "comment_text     object\n",
      "toxic             int64\n",
      "severe_toxic      int64\n",
      "obscene           int64\n",
      "threat            int64\n",
      "insult            int64\n",
      "identity_hate     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_text = pd.read_csv(\"train.csv\")\n",
    "print(train_text.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Explanation\\nWhy the edits made under my usern...\n",
      "1    \"\\nWelcome!\\n\\nHello, and welcome to Wikipedia...\n",
      "2    nb: might was well acknowledge that this was m...\n",
      "3                     László \\n\\nWhat is your problem?\n",
      "4    \"\\nHi, another few points about the second sub...\n",
      "Name: comment_text, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "comment = train_text['comment_text']\n",
    "print(comment.head())\n",
    "comment = comment.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
      "0      0             0        0       0       0              0\n",
      "1      0             0        0       0       0              0\n",
      "2      0             0        0       0       0              0\n",
      "3      0             0        0       0       0              0\n",
      "4      0             0        0       0       0              0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "label = train_text[['toxic', 'severe_toxic' , 'obscene' , 'threat' , 'insult' , 'identity_hate']]\n",
    "print(label.head())\n",
    "label = label.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "labels = []\n",
    "\n",
    "for ix in range(comment.shape[0]):\n",
    "    if len(comment[ix])<=400:\n",
    "        comments.append(comment[ix])\n",
    "        labels.append(label[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation\n",
    "import string\n",
    "print(string.punctuation)\n",
    "comments = [''.join(c for c in s if c not in string.punctuation) for s in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split tokens by white space\n",
    "comments = [word for line in comments for word in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    " # remove tokens that are not alphabetic\n",
    "comments = [comment for comment in comments if comment.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert letters to lower case\n",
    "comments = [comment.lower() for comment in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "comments = [comment for comment in comments if comment not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    " # remove short words (one letter)\n",
    "comments = [comment for comment in comments if len(comment) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "lem = WordNetLemmatizer()\n",
    "comments = [lem.lemmatize(comment,\"v\") for comment in comments]\n",
    "sentence = ' '.join(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "pos = nltk.pos_tag(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Preprocess                  POS\n",
      "0        explanation    (explanation, NN)\n",
      "1               edit          (edit, NNS)\n",
      "2               make          (make, VBP)\n",
      "3           username       (username, JJ)\n",
      "4           hardcore       (hardcore, NN)\n",
      "...              ...                  ...\n",
      "92966          funny          (funny, NN)\n",
      "92967           zora          (zora, NNS)\n",
      "92968           keep           (keep, VB)\n",
      "92969           look           (look, NN)\n",
      "92970  transgression  (transgression, NN)\n",
      "\n",
      "[92971 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "output = {'Preprocess' : comments,\n",
    "         'POS' : pos}\n",
    "df = DataFrame(output, columns= ['Preprocess', 'POS'])\n",
    "\n",
    "export_csv = df.to_csv(r\"export_dataframe.csv\", index = None, header=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
