Toxic comment classification
==============================

*To see the advancement please see the open, closed issues and [Project](https://github.com/mxmgllm/905-toxic_comment_classification/projects).*


Build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate.


## Authors
- [Maxime Guillaume](mailto:maxime.guillaume2@etu.univ-lorraine.fr)
- [Guilherme Razet](guilherme.razet@gmail.com)
- [Yi-ting Tsai](clara719tsai@gmail.com)
- [Sophea Ly](sophealy02@gmail.com)
## Project Organization
```
.
├── makefile
├── authors.md
├── readme.md
├── bin                <- Your compiled model code can be stored here (not tracked by git)
├── config             <- Configuration files, e.g., for doxygen or for your model if needed
├── data
│   ├── external       <- Data from third party sources.
│   ├── interim        <- Intermediate data that has been transformed.
│   ├── processed      <- The final, canonical data sets for modeling.
│   └── raw            <- The original, immutable data dump.
├── docs               <- Documentation, e.g., doxygen or scientific papers (not tracked by git)
├── notebooks          <- Ipython or R notebooks
├── report            <- For a manuscript source, e.g., LaTeX, Markdown, etc., or any project reports
│   └── figures        <- Figures for the manuscript or reports
└── src                <- Source code for this project
    ├── data           <- scripts and programs to process data
    ├── external       <- Any external source code, e.g., pull other git projects, or external libraries
    ├── models         <- Source code for your own model
    ├── tools          <- Any helper scripts go here
    └── visualization  <- Scripts for visualisation of your results, e.g., matplotlib, ggplot2 related.
```
